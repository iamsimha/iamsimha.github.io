---
layout: post
title: Generating Sentences from a Continuous Space
---
# [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)

Language, unlike an image and sound is discrete, becuase it is produced by intelligent beings and not by a physical phenomenon. It is interesting to think about, using a continuous latent variable to represent/encode a discete phenomenon like language.

For language modelling task autoregressive models like RNN's, LSTM's are the default tools. They are trained to predict the next word using a maximum likelihood objective. During prediction they only consider the previous word to generate the next word without accounting for the global sentence structure. In this paper the authors introduce a RNN based variational autoencoder, which utilizes a global distributed representation for generating sentences.


### Architecture
Encoder-Decoder architure is used for sequence autoencoding. But instead of using a bottleneck layer, VAE framework uses a normal random latent variable for decoding. 
Testing math rendering
$\Gamma(n) = (n-1)!\quad\forall
n\in\mathbb N$

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTM4NTg4OTExNF19
-->